# -*- coding: utf-8 -*-
"""Copy of fcc_predict_health_costs_with_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U_q0L4_8jkGrdlaNMUAZ6aonN19GFeCl

*Note: You are currently reading this using Google Colaboratory which is a cloud-hosted version of Jupyter Notebook. This is a document containing both text cells for documentation and runnable code cells. If you are unfamiliar with Jupyter Notebook, watch this 3-minute introduction before starting this challenge: https://www.youtube.com/watch?v=inN8seMm7UI*

---

In this challenge, you will predict healthcare costs using a regression algorithm.
You are given a dataset that contains information about different people including their healthcare costs. Use the data to predict healthcare costs based on new data.
The first two cells of this notebook import libraries and the data.
Make sure to convert categorical data to numbers. Use 80% of the data as the `train_dataset` and 20% of the data as the `test_dataset`.
`pop` off the "expenses" column from these datasets to create new datasets called `train_labels` and `test_labels`. Use these labels when training your model.
Create a model and train it with the `train_dataset`. Run the final cell in this notebook to check your model. The final cell will use the unseen `test_dataset` to check how well the model generalizes.
To pass the challenge, `model.evaluate` must return a Mean Absolute Error of under 3500. This means it predicts health care costs correctly within $3500.
The final cell will also predict expenses using the `test_dataset` and graph the results.
"""

# This python code is just the formatted code from the .ipynb file.
# This was formatted to provide a more detailed explanation for the code written
# Some of the variables might also renamed to provide a germane name
# Please run this on Google Colab, I did not test it on my local environment

################################### Code given by freeCodeCamp ###################################
# Commented out IPython magic to ensure Python compatibility.
# Import libraries. You may or may not use all of these.
!pip install -q git+https://github.com/tensorflow/docs
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
try:
  # %tensorflow_version only exists in Colab.
  %tensorflow_version 2.x
except Exception:
  pass
import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

import tensorflow_docs as tfdocs
import tensorflow_docs.plots
import tensorflow_docs.modeling
import tensorflow.compat.v2.feature_column as fc

# Import data
!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv
dataset = pd.read_csv('insurance.csv', encoding='utf-8')
dataset.tail()
################################### Code given by freeCodeCamp ###################################

################################### Code written by Apnatva Singh Rawat ###################################
import copy
from __future__ import absolute_import, division, print_function, unicode_literals

df = copy.deepcopy(dataset)
# df.head()
print(df.head())
# print(df[df.isnull().any(axis=1)])
# # df.fillna('', inplace=True)
# df.dropna(how="any",inplace = True)
# print(df[df.isnull().any(axis=1)])
df.info()
df.describe()

# changing the 'string' columns to int columns by encoding the features
df['smoker'] = [1 if i=='yes' else 0 for i in df['smoker']]
df['region'].unique() # found that there are 4 regions which are categorised as below
regionDict = {'southwest': 0, 'southeast': 1, 'northwest': 2, 'northeast': 3}
# encoding regions as this
df['region'] = [regionDict.get(i) for i in df['region']]
df['sex'] = [1 if i=='male' else 0 for i in df['sex']]
df.head()

# dividing 80% as training data. This can be any number.
train_dataset=df.sample(frac=0.8,random_state=10) #random state is a seed value
test_dataset=df.drop(train_dataset.index)
train_labels = train_dataset.pop('expenses')
test_labels = test_dataset.pop('expenses')

# normalizing the dataset to fit better to our dataset
# this step is optional
# but I found the model had greater accuracy when I normlaised it
norm = tf.keras.layers.Normalization(axis=-1)
norm.adapt(np.array(train_dataset))

model = tf.keras.Sequential([norm])
model.add(keras.layers.Dense(6,activation='relu'))
model.add(keras.layers.Dense(1))
model.summary()
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mean_absolute_error', metrics=['mean_absolute_error', 'mean_squared_error'])
# a lower learning rate worked better
# metrics as required by the freeCodeCamp

# a higher batch_size decreased the accuracy so I chose a smaller one with higher epochs
history = model.fit(train_dataset, train_labels, epochs=20, batch_size=5)
# using the previous split of the data we can run tests on it. But since fcc provided a function for this, I did not feel the need to test it on my owwn dataset
################################### Code written by Apnatva Singh Rawat ###################################

################################### Code given by freeCodeCamp for testing ###################################
# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.
# Test model by checking how well the model generalizes using the test set.
loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)
# loss, mae = model.evaluate(test_dataset, test_labels, verbose=2)

print("Testing set Mean Abs Error: {:5.2f} expenses".format(mae))

if mae < 3500:
  print("You passed the challenge. Great job!")
else:
  print("The Mean Abs Error must be less than 3500. Keep trying.")

# Plot predictions.
test_predictions = model.predict(test_dataset).flatten()

a = plt.axes(aspect='equal')
plt.scatter(test_labels, test_predictions)
plt.xlabel('True values (expenses)')
plt.ylabel('Predictions (expenses)')
lims = [0, 50000]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims,lims)
################################### Code given by freeCodeCamp for testing ###################################
